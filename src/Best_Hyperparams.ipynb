{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sazio/NMAs/blob/main/Data_Loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji-aTo7SA1AB"
   },
   "source": [
    "# Exploratory Data Analysis of Stringer Dataset \n",
    "@authors: Simone Azeglio, Chetan Dhulipalla , Khalid Saifullah \n",
    "\n",
    "\n",
    "Part of the code here has been taken from [Neuromatch Academy's Computational Neuroscience Course](https://compneuro.neuromatch.io/projects/neurons/README.html), and specifically from [this notebook](https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/projects/neurons/load_stringer_spontaneous.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2LTF6oe-U8R"
   },
   "source": [
    "# to do list\n",
    "\n",
    "1. custom normalization: dividing by mean value per neuron\n",
    "1a. downsampling: convolve then downsample by 5\n",
    "2. training validation split: withhold last 20 percent of time series for testing\n",
    "3. RNN for each layer: a way to capture the dynamics inside each layer instead of capturing extra dynamics from inter-layer interactions. it will be OK to compare the different RNNs. maintain same neuron count in each layer to reduce potential bias \n",
    "4. layer weight regularization: L2 \n",
    "5. early stopping , dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install wandb --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs7e5ppCMYCK"
   },
   "source": [
    "## Loading of Stringer spontaneous data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "u0nA90QhJurD"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "import os, requests\n",
    "\n",
    "fname = \"stringer_spontaneous.npy\"\n",
    "url = \"https://osf.io/dpqaj/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FgbdwXWDSUpO"
   },
   "outputs": [],
   "source": [
    "#@title Import matplotlib and set defaults\n",
    "from matplotlib import rcParams \n",
    "from matplotlib import pyplot as plt\n",
    "rcParams['figure.figsize'] = [20, 4]\n",
    "rcParams['font.size'] =15\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRWWoEX0-sYp"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ulJ34TyRZo6",
    "outputId": "df56f72e-e25f-4702-b021-bf6959e018c7"
   },
   "outputs": [],
   "source": [
    "#@title Data loading\n",
    "import numpy as np\n",
    "dat = np.load('stringer_spontaneous.npy', allow_pickle=True).item()\n",
    "print(dat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGn2iJGmFpLC"
   },
   "outputs": [],
   "source": [
    "# functions \n",
    "\n",
    "def moving_avg(array, factor = 5):\n",
    "    \"\"\"Reducing the number of compontents by averaging of N = factor\n",
    "    subsequent elements of array\"\"\"\n",
    "    zeros_ = np.zeros((array.shape[0], 2))\n",
    "    array = np.hstack((array, zeros_))\n",
    "\n",
    "    array = np.reshape(array, (array.shape[0],  int(array.shape[1]/factor), factor))\n",
    "    array = np.mean(array, axis = 2)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdjYTZeV-yhR"
   },
   "source": [
    "## Extracting Data for RNN (or LFADS)\n",
    "The first problem to address is that for each layer we don't have the exact same number of neurons. We'd like to have a single RNN encoding all the different layers activities, to make it easier we can take the number of neurons ($N_{neurons} = 1131$ of the least represented class (layer) and level out each remaining class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEy_qiyKY1xG"
   },
   "outputs": [],
   "source": [
    "# Extract labels from z - coordinate\n",
    "from sklearn import preprocessing\n",
    "x, y, z = dat['xyz']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(z)\n",
    "### least represented class (layer with less neurons)\n",
    "n_samples = np.histogram(labels, bins=9)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lb3M2PSOZpMW"
   },
   "outputs": [],
   "source": [
    "### Data for LFADS / RNN \n",
    "import pandas as pd \n",
    "dataSet = pd.DataFrame(dat[\"sresp\"])\n",
    "dataSet[\"label\"] = labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYThvxV-2Nl6"
   },
   "outputs": [],
   "source": [
    "# it can be done in one loop ... \n",
    "data_ = []\n",
    "for i in range(0, 9):\n",
    "    data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n",
    "\n",
    "dataRNN = np.zeros((n_samples*9, dataSet.shape[1]-1))\n",
    "for i in range(0,9):\n",
    "    \n",
    "    # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n",
    "    ## normalized by layer\n",
    "    dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n",
    "\n",
    "## shuffling for training purposes\n",
    "\n",
    "#np.random.shuffle(dataRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WAqcnCTrZAz"
   },
   "outputs": [],
   "source": [
    "#unshuffled = np.array(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "680ch36_-e0m"
   },
   "outputs": [],
   "source": [
    "#@title Convolutions code\n",
    "\n",
    "# convolution moving average\n",
    "\n",
    "# kernel_length = 50\n",
    "# averaging_kernel = np.ones(kernel_length) / kernel_length\n",
    "\n",
    "# dataRNN.shape\n",
    "\n",
    "# avgd_dataRNN = list()\n",
    "\n",
    "# for neuron in dataRNN:\n",
    "#   avgd_dataRNN.append(np.convolve(neuron, averaging_kernel))\n",
    "\n",
    "# avg_dataRNN = np.array(avgd_dataRNN)\n",
    "\n",
    "# print(avg_dataRNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "63S3144a31FI"
   },
   "outputs": [],
   "source": [
    "# @title Z Score Code \n",
    "\n",
    "\n",
    "# from scipy.stats import zscore\n",
    "\n",
    "\n",
    "# neuron = 500\n",
    "\n",
    "# scaled_all = zscore(avg_dataRNN)\n",
    "# scaled_per_neuron = zscore(avg_dataRNN[neuron, :])\n",
    "\n",
    "# scaled_per_layer = list()\n",
    "\n",
    "# for layer in unshuffled:\n",
    "#   scaled_per_layer.append(zscore(layer))\n",
    "\n",
    "# scaled_per_layer = np.array(scaled_per_layer)\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(avg_dataRNN[neuron, :])\n",
    "# plt.plot(avg_dataRNN[2500, :])\n",
    "# plt.figure()\n",
    "# plt.plot(dataRNN[neuron, :])\n",
    "# plt.figure()\n",
    "# plt.plot(scaled_all[neuron, :])\n",
    "# plt.plot(scaled_per_neuron)\n",
    "# plt.figure()\n",
    "# plt.plot(scaled_per_layer[0,neuron,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE-4w_4V-Gzx"
   },
   "outputs": [],
   "source": [
    "# custom normalization\n",
    "\n",
    "normed_dataRNN = list()\n",
    "for neuron in dataRNN:\n",
    "    normed_dataRNN.append(neuron)# / neuron.mean())\n",
    "normed_dataRNN = np.array(normed_dataRNN)\n",
    "\n",
    "# downsampling and averaging \n",
    "#avgd_normed_dataRNN = dataRNN#\n",
    "avgd_normed_dataRNN = moving_avg(dataRNN, factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avgd_normed_dataRNN[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeNxo6vsv1Oq"
   },
   "source": [
    "issue: does the individual scaling by layer introduce bias that may artificially increase performance of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MrXC5QIiyhJ"
   },
   "source": [
    "## Data Loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "255tz5iqmSq1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "467WOHhtmXmb"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dcb5vW2joVW_"
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# number of neurons \n",
    "NN = dataRNN.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXJPQdu17Ns5",
    "outputId": "18b700ff-0126-458f-a300-7488e457013f"
   },
   "outputs": [],
   "source": [
    "# swapping the axes to maintain consistency with seq2seq notebook in the following code - the network takes all the neurons at a time step as input, not just one neuron\n",
    "\n",
    "# avgd_normed_dataRNN = np.swapaxes(avgd_normed_dataRNN, 0, 1)\n",
    "avgd_normed_dataRNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prl0OxLZkka9"
   },
   "outputs": [],
   "source": [
    "frac = 4/5\n",
    "\n",
    "#x1 = torch.from_numpy(dataRNN[:,:int(frac*dataRNN.shape[1])]).to(device).float().unsqueeze(0)\n",
    "#x2 = torch.from_numpy(dataRNN[:,int(frac*dataRNN.shape[1]):]).to(device).float().unsqueeze(0)\n",
    "#x1 = torch.from_numpy(avgd_normed_dataRNN[:1131,:]).to(device).float().unsqueeze(2)\n",
    "#x2 = torch.from_numpy(avgd_normed_dataRNN[:1131,:]).to(device).float().unsqueeze(2)\n",
    "\n",
    "n_neurs = 1131\n",
    "# let's use n_neurs/10 latent components\n",
    "ncomp = int(n_neurs/10)\n",
    "\n",
    "x1_train = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,:int(frac*avgd_normed_dataRNN.shape[1])]).to(device).float().unsqueeze(2)\n",
    "x2_train = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,:int(frac*avgd_normed_dataRNN.shape[1])]).to(device).float().unsqueeze(2)\n",
    "\n",
    "x1_valid = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,int(frac*avgd_normed_dataRNN.shape[1]):]).to(device).float().unsqueeze(2)\n",
    "x2_valid = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,int(frac*avgd_normed_dataRNN.shape[1]):]).to(device).float().unsqueeze(2)\n",
    "\n",
    "NN1 = x1_train.shape[0]\n",
    "NN2 = x2_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01IvhjPzk-Jw"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ncomp, NN1, NN2, num_layers = 1, n_comp = 50, dropout= 0,  bidi=True):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # play with some of the options in the RNN!\n",
    "        \n",
    "        self.rnn = nn.LSTM(NN1, ncomp, num_layers = num_layers, dropout = dropout,\n",
    "                         bidirectional = bidi)\n",
    "        \"\"\"\n",
    "        self.rnn = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0,\n",
    "                    bidirectional = bidi, nonlinearity = 'tanh')\n",
    "        self.rnn = nn.GRU(NN1, ncomp, num_layers = 1, dropout = 0,\n",
    "                         bidirectional = bidi)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "                    nn.Linear(ncomp, ncomp*2),\n",
    "                    nn.Mish(),\n",
    "                    nn.Linear(ncomp*2, ncomp*2),\n",
    "                    nn.Mish(),\n",
    "                    nn.Dropout(0.25),\n",
    "                    nn.Linear(ncomp*2, ncomp), \n",
    "                    nn.Mish())\n",
    "        \n",
    "        self.fc = nn.Linear(ncomp, NN2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 2, 0)\n",
    "        #print(x.shape)\n",
    "        # h_0 = torch.zeros(2, x.size()[1], self.ncomp).to(device)\n",
    "        \n",
    "        y, h_n = self.rnn(x)\n",
    "\n",
    "        #print(y.shape)\n",
    "        #print(h_n.shape)\n",
    "        if self.rnn.bidirectional:\n",
    "          # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n",
    "          # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n",
    "            q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n",
    "        else:\n",
    "            q = y\n",
    "        \n",
    "        q = self.mlp(q)\n",
    "\n",
    "        # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n",
    "        # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n",
    "        #z = F.softplus(self.fc(q), 10)\n",
    "        #print(q.shape)\n",
    "        z = self.fc(q).permute(2, 0, 1)\n",
    "        # print(z.shape)\n",
    "        return z, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam']\n",
    "        },\n",
    "    'num_layers': {\n",
    "        'values': [1]\n",
    "        },\n",
    "    'n_comp': {\n",
    "        'values': [75]\n",
    "        },\n",
    "    \n",
    "    'dropout': {\n",
    "          'values': [0.2]\n",
    "        },\n",
    "    'weight_decay': {\n",
    "          'values': [1e-6]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 50000}\n",
    "    })\n",
    "\n",
    "import math\n",
    "\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': -9.9,\n",
    "        'max': -5.3\n",
    "    },\n",
    "})\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"NMAs-Best-Params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can keep re-running this cell if you think the cost might decrease further\n",
    "cost = nn.MSELoss()\n",
    "\n",
    "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hlfx8Ltp5jJ"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjzD06B9ta3n"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, weight_decay): \n",
    "    optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # loader = build_dataset(config.batch_size)\n",
    "        # Net(ncomp, NN1, NN2, bidi = True).to(device)\n",
    "        network = Net(ncomp, NN1, NN2, config.num_layers, config.dropout).to(device)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            # avg_loss = train_epoch(network, loader, optimizer)\n",
    "            network.train()\n",
    "            # the networkwork outputs the single-neuron prediction and the latents\n",
    "            z, y = network(x1_train)\n",
    "\n",
    "            # our cost\n",
    "            loss = cost(z, x2_train)\n",
    "\n",
    "            # train the networkwork as usual\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                network.eval()\n",
    "                valid_loss = cost(network(x1_valid)[0], x2_valid)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                with torch.no_grad():\n",
    "                    network.eval()\n",
    "                    valid_loss = cost(network(x1_valid)[0], x2_valid)\n",
    "                    \n",
    "                    print(f' iteration {epoch}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')\n",
    "            wandb.log({\"train_loss\": loss.item(), 'valid_loss': valid_loss.item(), \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count= 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Data_Loader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "32dcf54089bc45cbbc7ace38a12d88ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e573a939d0c445b9e11bdcaf3c8098b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e47ef23e2b144ea3ac0d313d52ab158d",
       "IPY_MODEL_6b88c683452f46869fa8d3431ce8c0f2"
      ],
      "layout": "IPY_MODEL_a22204bd65224165bf9a0ea65d1e5592"
     }
    },
    "5dd4e738c49542eab4ee6833cc62b146": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b88c683452f46869fa8d3431ce8c0f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8598ad26b4ee4da58c65a2b2741cd880",
      "placeholder": "​",
      "style": "IPY_MODEL_5dd4e738c49542eab4ee6833cc62b146",
      "value": " 1000/1000 [04:50&lt;00:00,  3.44it/s]"
     }
    },
    "8598ad26b4ee4da58c65a2b2741cd880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a22204bd65224165bf9a0ea65d1e5592": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d626a6d0d4ca442da18e75016ca4603e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e47ef23e2b144ea3ac0d313d52ab158d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32dcf54089bc45cbbc7ace38a12d88ff",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d626a6d0d4ca442da18e75016ca4603e",
      "value": 1000
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
