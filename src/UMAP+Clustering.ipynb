{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/sazio/NMAs/blob/main/src/UMAP%2BClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# UMAP + Clustering on Raw Data vs RNNs Hidden Representations\n@authors: Simone Azeglio, Chetan Dhulipalla , Khalid Saifullah \n\n\nPart of the code here has been taken from [Neuromatch Academy's Computational Neuroscience Course](https://compneuro.neuromatch.io/projects/neurons/README.html), and specifically from [this notebook](https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/projects/neurons/load_stringer_spontaneous.ipynb)","metadata":{"id":"ji-aTo7SA1AB"}},{"cell_type":"markdown","source":"## Loading of Stringer spontaneous data\n\n","metadata":{"id":"vs7e5ppCMYCK"}},{"cell_type":"code","source":"#@title Data retrieval\nimport os, requests\n\nfname = \"stringer_spontaneous.npy\"\nurl = \"https://osf.io/dpqaj/download\"\n\nif not os.path.isfile(fname):\n    try:\n        r = requests.get(url)\n    except requests.ConnectionError:\n        print(\"!!! Failed to download data !!!\")\n    else:\n        if r.status_code != requests.codes.ok:\n            print(\"!!! Failed to download data !!!\")\n        else:\n            with open(fname, \"wb\") as fid:\n                fid.write(r.content)","metadata":{"cellView":"form","id":"u0nA90QhJurD","execution":{"iopub.status.busy":"2021-08-18T21:49:44.128759Z","iopub.execute_input":"2021-08-18T21:49:44.129261Z","iopub.status.idle":"2021-08-18T21:49:44.141302Z","shell.execute_reply.started":"2021-08-18T21:49:44.129173Z","shell.execute_reply":"2021-08-18T21:49:44.139930Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#@title Import matplotlib and set defaults\nfrom matplotlib import rcParams \nfrom matplotlib import pyplot as plt\nrcParams['figure.figsize'] = [20, 4]\nrcParams['font.size'] =15\nrcParams['axes.spines.top'] = False\nrcParams['axes.spines.right'] = False\nrcParams['figure.autolayout'] = True","metadata":{"cellView":"form","id":"FgbdwXWDSUpO","execution":{"iopub.status.busy":"2021-08-18T21:49:44.143234Z","iopub.execute_input":"2021-08-18T21:49:44.143737Z","iopub.status.idle":"2021-08-18T21:49:44.151798Z","shell.execute_reply.started":"2021-08-18T21:49:44.143701Z","shell.execute_reply":"2021-08-18T21:49:44.150958Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{"id":"SRWWoEX0-sYp"}},{"cell_type":"code","source":"#@title Data loading\nimport numpy as np\ndat = np.load('stringer_spontaneous.npy', allow_pickle=True).item()\nprint(dat.keys())","metadata":{"id":"6ulJ34TyRZo6","outputId":"88ccc7ef-ccf9-4a0e-8e6c-c315309ad9cf","execution":{"iopub.status.busy":"2021-08-18T21:49:52.893091Z","iopub.execute_input":"2021-08-18T21:49:52.893421Z","iopub.status.idle":"2021-08-18T21:49:57.334754Z","shell.execute_reply.started":"2021-08-18T21:49:52.893390Z","shell.execute_reply":"2021-08-18T21:49:57.333790Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"dict_keys(['sresp', 'run', 'beh_svd_time', 'beh_svd_mask', 'stat', 'pupilArea', 'pupilCOM', 'xyz'])\n","output_type":"stream"}]},{"cell_type":"code","source":"# functions \n\ndef moving_avg(array, factor = 5):\n    \"\"\"Reducing the number of compontents by averaging of N = factor\n    subsequent elements of array\"\"\"\n    zeros_ = np.zeros((array.shape[0], 2))\n    array = np.hstack((array, zeros_))\n\n    array = np.reshape(array, (array.shape[0],  int(array.shape[1]/factor), factor))\n    array = np.mean(array, axis = 2)\n\n    return array","metadata":{"id":"KGn2iJGmFpLC","execution":{"iopub.status.busy":"2021-08-18T21:49:57.336264Z","iopub.execute_input":"2021-08-18T21:49:57.336819Z","iopub.status.idle":"2021-08-18T21:49:57.343344Z","shell.execute_reply.started":"2021-08-18T21:49:57.336779Z","shell.execute_reply":"2021-08-18T21:49:57.342419Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Extracting Data for RNN (or LFADS)\nThe first problem to address is that for each layer we don't have the exact same number of neurons. We'd like to have a single RNN encoding all the different layers activities, to make it easier we can take the number of neurons ($N_{neurons} = 1131$ of the least represented class (layer) and level out each remaining class. ","metadata":{"id":"ZdjYTZeV-yhR"}},{"cell_type":"code","source":"# Extract labels from z - coordinate\nfrom sklearn import preprocessing\nx, y, z = dat['xyz']\n\nle = preprocessing.LabelEncoder()\nlabels = le.fit_transform(z)\n### least represented class (layer with less neurons)\nn_samples = np.histogram(labels, bins=9)[0][-1]","metadata":{"id":"mEy_qiyKY1xG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Data for LFADS / RNN \nimport pandas as pd \ndataSet = pd.DataFrame(dat[\"sresp\"])\ndataSet[\"label\"] = labels ","metadata":{"id":"lb3M2PSOZpMW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it can be done in one loop ... \ndata_ = []\n\nfor i in range(0, 9):\n    data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n\ndataRNN = np.zeros((n_samples*9, dataSet.shape[1]-1))\nfor i in range(0,9):\n    \n    # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n    ## normalized by layer\n    dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n\n## shuffling for training purposes\n\n#np.random.shuffle(dataRNN)","metadata":{"id":"fYThvxV-2Nl6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# downsampling and averaging \n#avgd_normed_dataRNN = dataRNN#\navgd_normed_dataRNN = moving_avg(dataRNN, factor=2)","metadata":{"id":"hE-4w_4V-Gzx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(avgd_normed_dataRNN[0,:])","metadata":{"id":"rIoBNGiFHZFy","outputId":"50e60a1b-fff7-443c-bb8e-c576e3dc5348","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  UMAP Visualization (Semi-Supervised) on Raw Data\nThe result is not encouraging, let's see if the RNN has captured some meaningful dynamics and UMAP can extract it in 2D.","metadata":{"id":"_jkq_bbiai6N"}},{"cell_type":"code","source":"!pip install umap-learn hdbscan --quiet","metadata":{"id":"ZW8NnGx-Y79L","outputId":"487ee08a-33b7-4af8-8c4b-83d992e3aa27","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dimension reduction and clustering libraries\nimport umap\n#import hdbscan\nimport sklearn.cluster as cluster\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"vb88rV2rZr5I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = np.ones(1131)\nlabels = np.hstack((0*b,b, 2*b, 3*b,4*b, 5*b, 6*b, 7*b, 8*b))","metadata":{"id":"nWkAlMAakuGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,  X_test, y_train, y_test = train_test_split(avgd_normed_dataRNN, labels, test_size = 0.10, random_state = 2021)","metadata":{"id":"SiWxjCGTo4op"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#factor = 6 \nemb = umap.UMAP(random_state = 2021, n_components=2, n_neighbors= 45, min_dist = 0.3).fit(X_train, y = y_train)","metadata":{"id":"rsoaXcOidOIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embedding = emb.transform(X_test)","metadata":{"id":"GO8Sez4vpYYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 8))\n#ax = fig.add_subplot(projection='3d')\n#plt.scatter(emb[:1131*factor,0], emb[:1131*factor,1], emb[:1131*factor, 2],  c = labels ,cmap = 'Spectral')\nplt.scatter(*emb.embedding_.T,  c = y_train ,cmap = 'Spectral')\ncbar = plt.colorbar(boundaries=np.arange(9))\ncbar.set_ticks(np.arange(9))\ncbar.set_ticklabels([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"])","metadata":{"id":"0aR4W3JrenjH","outputId":"ab52ef39-ccd9-4566-e5c3-8780d75958c2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (15, 8))\n#ax = fig.add_subplot(projection='3d')\n#plt.scatter(emb[:1131*factor,0], emb[:1131*factor,1], emb[:1131*factor, 2],  c = labels ,cmap = 'Spectral')\nplt.scatter(*test_embedding.T,  c = y_test ,cmap = 'Spectral')\ncbar = plt.colorbar(boundaries=np.arange(9))\ncbar.set_ticks(np.arange(9))\ncbar.set_ticklabels([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"])","metadata":{"id":"UdLCmDq-qyIk","outputId":"d90ce320-0486-4c17-e97c-694f0c845973"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UMAP on RNN's Reduced dynamics\n\n### Extract Dynamics\n","metadata":{"id":"r6dCfpxOCDoY"}},{"cell_type":"code","source":"# Extract labels from z - coordinate\nfrom sklearn import preprocessing\nx, y, z = dat['xyz']\n\nle = preprocessing.LabelEncoder()\nlabels = le.fit_transform(z)\n### least represented class (layer with less neurons)\nn_samples = np.histogram(labels, bins=9)[0][-1]","metadata":{"id":"vtPmg7XnL0X4","execution":{"iopub.status.busy":"2021-08-18T21:49:57.868198Z","iopub.execute_input":"2021-08-18T21:49:57.868489Z","iopub.status.idle":"2021-08-18T21:49:58.379430Z","shell.execute_reply.started":"2021-08-18T21:49:57.868463Z","shell.execute_reply":"2021-08-18T21:49:58.378598Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"### Data for LFADS / RNN \nimport pandas as pd \ndataSet = pd.DataFrame(dat[\"sresp\"])\ndataSet[\"label\"] = labels ","metadata":{"id":"3PPgUYo4L-5i","execution":{"iopub.status.busy":"2021-08-18T21:49:58.380948Z","iopub.execute_input":"2021-08-18T21:49:58.381312Z","iopub.status.idle":"2021-08-18T21:49:58.387189Z","shell.execute_reply.started":"2021-08-18T21:49:58.381273Z","shell.execute_reply":"2021-08-18T21:49:58.386410Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# it can be done in one loop ... \ndata_ = []\nfor i in range(0, 9):\n    data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n\ndataRNN = np.zeros((n_samples, dataSet.shape[1]-1, 9))\nfor i in range(0,9):\n    \n    # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n    ## normalized by layer\n    dataRNN[:, :, i] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n","metadata":{"id":"htkpTS0DMD66","execution":{"iopub.status.busy":"2021-08-18T21:49:58.388696Z","iopub.execute_input":"2021-08-18T21:49:58.389169Z","iopub.status.idle":"2021-08-18T21:50:01.569628Z","shell.execute_reply.started":"2021-08-18T21:49:58.389133Z","shell.execute_reply":"2021-08-18T21:50:01.568743Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# functions \n\ndef moving_avg(array, factor = 2):\n    \"\"\"Reducing the number of compontents by averaging of N = factor\n    subsequent elements of array\"\"\"\n    #zeros_ = np.zeros((array.shape[0], 2))\n    #array = np.hstack((array, zeros_))\n    \n    array = np.reshape(array, (array.shape[0],  int(array.shape[1]/factor), factor, array.shape[2]))\n    array = np.mean(array, axis = 2)\n\n    return array","metadata":{"id":"qhJjHE9PMD9U","execution":{"iopub.status.busy":"2021-08-18T21:50:01.571274Z","iopub.execute_input":"2021-08-18T21:50:01.571671Z","iopub.status.idle":"2021-08-18T21:50:01.576979Z","shell.execute_reply.started":"2021-08-18T21:50:01.571619Z","shell.execute_reply":"2021-08-18T21:50:01.576037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# downsampling and averaging \navgd_normed_dataRNN = moving_avg(dataRNN, factor=2)","metadata":{"id":"jPxR40BYMEAK","execution":{"iopub.status.busy":"2021-08-18T21:50:01.578769Z","iopub.execute_input":"2021-08-18T21:50:01.579326Z","iopub.status.idle":"2021-08-18T21:50:02.190980Z","shell.execute_reply.started":"2021-08-18T21:50:01.579286Z","shell.execute_reply":"2021-08-18T21:50:02.189951Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#!pip3 install torch --upgrade","metadata":{"execution":{"iopub.status.busy":"2021-08-18T21:50:02.192513Z","iopub.execute_input":"2021-08-18T21:50:02.192894Z","iopub.status.idle":"2021-08-18T21:50:02.196879Z","shell.execute_reply.started":"2021-08-18T21:50:02.192854Z","shell.execute_reply":"2021-08-18T21:50:02.196071Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.cuda.empty_cache()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.__version__) # should be 1.9.0 for training (Mish Activation)","metadata":{"id":"255tz5iqmSq1","execution":{"iopub.status.busy":"2021-08-18T21:50:02.198560Z","iopub.execute_input":"2021-08-18T21:50:02.199208Z","iopub.status.idle":"2021-08-18T21:50:02.739057Z","shell.execute_reply.started":"2021-08-18T21:50:02.199142Z","shell.execute_reply":"2021-08-18T21:50:02.737639Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"1.9.0+cu102\n","output_type":"stream"}]},{"cell_type":"code","source":"# set the seed\nnp.random.seed(42)\n\n# number of neurons \nNN = dataRNN.shape[0]","metadata":{"id":"Dcb5vW2joVW_","execution":{"iopub.status.busy":"2021-08-18T21:50:06.646182Z","iopub.execute_input":"2021-08-18T21:50:06.646519Z","iopub.status.idle":"2021-08-18T21:50:06.651168Z","shell.execute_reply.started":"2021-08-18T21:50:06.646487Z","shell.execute_reply":"2021-08-18T21:50:06.650237Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"frac = 4/5\n#n_neurs = 1131\n# let's use n_neurs/10 latent components\nncomp = 75 #int(n_neurs/10)\n\nx1_train = torch.from_numpy(avgd_normed_dataRNN[:,:int(frac*avgd_normed_dataRNN.shape[1]), : ]).to(device).float()\nx2_train = torch.from_numpy(avgd_normed_dataRNN[:,:int(frac*avgd_normed_dataRNN.shape[1]),: ]).to(device).float()\n\nx1_valid = torch.from_numpy(avgd_normed_dataRNN[:,int(frac*avgd_normed_dataRNN.shape[1]):, :] ).to(device).float()\nx2_valid = torch.from_numpy(avgd_normed_dataRNN[:,int(frac*avgd_normed_dataRNN.shape[1]):, :]).to(device).float()\n\nNN1 = x1_train.shape[0]\nNN2 = x2_train.shape[0]\n","metadata":{"id":"Prl0OxLZkka9","execution":{"iopub.status.busy":"2021-08-18T21:50:07.120938Z","iopub.execute_input":"2021-08-18T21:50:07.121261Z","iopub.status.idle":"2021-08-18T21:50:10.468511Z","shell.execute_reply.started":"2021-08-18T21:50:07.121232Z","shell.execute_reply":"2021-08-18T21:50:10.467609Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, ncomp, NN1, NN2, bidi=True):\n        super(Net, self).__init__()\n\n        # play with some of the options in the RNN!\n        \n        self.rnn = nn.LSTM(NN1, ncomp, num_layers = 1, dropout = 0.3,\n                         bidirectional = bidi)\n        \"\"\"\n        self.rnn = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0,\n                    bidirectional = bidi, nonlinearity = 'tanh')\n        self.rnn = nn.GRU(NN1, ncomp, num_layers = 1, dropout = 0,\n                         bidirectional = bidi)\n        \"\"\"\n        \n        self.mlp = nn.Sequential(\n                    nn.Linear(ncomp, ncomp*2),\n                    nn.Mish(),\n                    nn.Linear(ncomp*2, ncomp*2),\n                    nn.Mish(),\n                    nn.Dropout(0.25),\n                    nn.Linear(ncomp*2, ncomp), \n                    nn.Mish())\n        \n        self.fc = nn.Linear(ncomp, NN2)\n\n    def forward(self, x):\n        x = x.permute(1, 2, 0)\n        #print(x.shape)\n        # h_0 = torch.zeros(2, x.size()[1], self.ncomp).to(device)\n        \n        y, h_n = self.rnn(x)\n\n        #print(y.shape)\n        #print(h_n.shape)\n        if self.rnn.bidirectional:\n          # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n          # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n            q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n        else:\n            q = y\n        \n        q = self.mlp(q)\n\n        # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n        # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n        #z = F.softplus(self.fc(q), 10)\n        #print(q.shape)\n        z = self.fc(q).permute(2, 0, 1)\n        # print(z.shape)\n        return z, q","metadata":{"id":"01IvhjPzk-Jw","execution":{"iopub.status.busy":"2021-08-18T22:01:36.542104Z","iopub.execute_input":"2021-08-18T22:01:36.542571Z","iopub.status.idle":"2021-08-18T22:01:36.558030Z","shell.execute_reply.started":"2021-08-18T22:01:36.542530Z","shell.execute_reply":"2021-08-18T22:01:36.557075Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# we initialize the neural network\nnet = Net(ncomp, NN1, NN2, bidi = True).to(device)\n\n# we set up the optimizer. Adjust the learning rate if the training is slow or if it explodes.\n# optimizer1 = torch.optim.Adam(net.parameters(), lr= 0.0003795, weight_decay= 10e-6)","metadata":{"id":"sQGEVQaGmwV6","outputId":"fcc85147-84de-42cc-938a-91051fe7f393","execution":{"iopub.status.busy":"2021-08-18T22:01:36.881391Z","iopub.execute_input":"2021-08-18T22:01:36.881692Z","iopub.status.idle":"2021-08-18T22:01:36.898595Z","shell.execute_reply.started":"2021-08-18T22:01:36.881643Z","shell.execute_reply":"2021-08-18T22:01:36.897765Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  \"num_layers={}\".format(dropout, num_layers))\n","output_type":"stream"}]},{"cell_type":"code","source":"# forward check \n# net(x1)\nnet(x1_train)","metadata":{"id":"SzvKjuTvmwYa","outputId":"aa317773-32e0-4c60-e8ef-2d0e04df211b","execution":{"iopub.status.busy":"2021-08-18T22:01:37.305034Z","iopub.execute_input":"2021-08-18T22:01:37.305353Z","iopub.status.idle":"2021-08-18T22:01:37.440024Z","shell.execute_reply.started":"2021-08-18T22:01:37.305324Z","shell.execute_reply":"2021-08-18T22:01:37.439085Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(tensor([[[ 1.2995e-01,  1.1363e-01,  1.1584e-01,  ...,  1.1691e-01,\n            1.2452e-01,  1.2659e-01],\n          [ 1.2849e-01,  1.0682e-01,  1.1480e-01,  ...,  1.1263e-01,\n            1.2770e-01,  1.3833e-01],\n          [ 1.2913e-01,  1.2242e-01,  1.1338e-01,  ...,  1.2603e-01,\n            1.2066e-01,  1.3443e-01],\n          ...,\n          [ 1.3281e-01,  1.1721e-01,  1.2239e-01,  ...,  1.4201e-01,\n            1.2252e-01,  1.3039e-01],\n          [ 1.1495e-01,  1.2263e-01,  1.1979e-01,  ...,  1.2739e-01,\n            1.2342e-01,  1.3211e-01],\n          [ 1.2958e-01,  1.2760e-01,  1.2604e-01,  ...,  1.2060e-01,\n            1.1678e-01,  1.3745e-01]],\n \n         [[-9.4108e-04,  2.5550e-03,  1.0541e-02,  ...,  6.9388e-03,\n            1.0219e-02,  1.5206e-02],\n          [ 3.5089e-03,  1.9745e-03,  8.2637e-03,  ...,  1.8807e-03,\n            8.6694e-03,  1.5735e-02],\n          [ 7.1012e-03, -5.5105e-03,  2.0005e-03,  ...,  5.7611e-03,\n            1.6753e-02,  1.9461e-02],\n          ...,\n          [ 1.4847e-02,  5.0227e-03,  9.3901e-03,  ...,  7.7693e-03,\n           -3.0659e-03,  1.0927e-02],\n          [ 3.6413e-03, -1.2634e-02,  2.0307e-03,  ...,  4.2335e-03,\n           -1.5375e-03,  6.6935e-03],\n          [ 5.2125e-03,  6.0116e-03,  7.9501e-03,  ...,  2.4266e-03,\n            6.5452e-05,  1.3562e-02]],\n \n         [[ 8.6314e-02,  1.0269e-01,  1.0268e-01,  ...,  1.0239e-01,\n            1.0152e-01,  9.2070e-02],\n          [ 8.5004e-02,  9.3279e-02,  8.7421e-02,  ...,  1.0868e-01,\n            8.9814e-02,  1.0038e-01],\n          [ 8.5901e-02,  8.9222e-02,  9.0406e-02,  ...,  1.0239e-01,\n            1.0403e-01,  1.1358e-01],\n          ...,\n          [ 1.0061e-01,  1.0577e-01,  9.3449e-02,  ...,  9.0617e-02,\n            9.5506e-02,  1.1058e-01],\n          [ 1.0389e-01,  1.0275e-01,  9.6247e-02,  ...,  1.0763e-01,\n            1.0069e-01,  1.0624e-01],\n          [ 9.7119e-02,  9.9813e-02,  9.3074e-02,  ...,  9.8028e-02,\n            9.0317e-02,  9.5884e-02]],\n \n         ...,\n \n         [[ 5.7626e-03,  4.8985e-03,  1.1760e-02,  ...,  2.2288e-02,\n            1.6828e-02,  9.7525e-03],\n          [ 9.2933e-03,  1.4342e-02,  1.9673e-02,  ...,  2.3137e-02,\n            6.9593e-03,  1.3091e-02],\n          [ 4.7600e-03,  1.6818e-03,  1.4738e-02,  ...,  1.0376e-02,\n            2.3529e-02,  1.3493e-02],\n          ...,\n          [-9.3239e-03,  3.2942e-03,  6.4152e-03,  ...,  2.8889e-02,\n            1.0420e-02,  2.2050e-02],\n          [ 9.1816e-03,  1.3060e-02,  4.5609e-03,  ...,  1.1911e-02,\n            1.9467e-02,  2.5648e-02],\n          [-1.2856e-04,  2.1394e-03,  2.5957e-02,  ...,  1.3880e-02,\n            7.0375e-03,  4.3271e-03]],\n \n         [[-1.6791e-02, -1.2745e-02, -8.6171e-03,  ..., -3.7582e-02,\n           -9.1489e-03, -1.6514e-02],\n          [-1.0605e-02, -1.2174e-02, -1.0478e-02,  ..., -1.8377e-02,\n           -1.6981e-02, -3.0575e-02],\n          [-4.7579e-03, -2.0476e-02, -1.3966e-02,  ..., -1.6255e-02,\n           -1.9459e-02, -1.0144e-02],\n          ...,\n          [-1.8086e-02, -2.5801e-02, -1.8927e-02,  ..., -3.7726e-02,\n            9.6199e-04, -2.8996e-02],\n          [-7.7547e-03, -2.2019e-02, -1.4539e-02,  ..., -1.2109e-02,\n           -1.5621e-02, -1.8299e-02],\n          [-2.7929e-03, -2.1162e-02, -1.0027e-02,  ..., -1.5471e-02,\n           -8.2940e-03, -1.5076e-02]],\n \n         [[ 8.7070e-02,  9.8959e-02,  9.1582e-02,  ...,  9.1884e-02,\n            8.5793e-02,  9.4441e-02],\n          [ 9.2408e-02,  1.0061e-01,  8.6618e-02,  ...,  9.5740e-02,\n            8.7600e-02,  9.4623e-02],\n          [ 8.9048e-02,  9.5764e-02,  8.9296e-02,  ...,  9.9734e-02,\n            9.5805e-02,  8.4168e-02],\n          ...,\n          [ 8.9553e-02,  8.5385e-02,  7.4991e-02,  ...,  9.0047e-02,\n            8.7635e-02,  9.3155e-02],\n          [ 9.1066e-02,  8.8404e-02,  7.4055e-02,  ...,  8.8798e-02,\n            8.7804e-02,  9.0858e-02],\n          [ 8.7237e-02,  9.3192e-02,  9.4325e-02,  ...,  8.9424e-02,\n            8.8807e-02,  8.9559e-02]]], device='cuda:0',\n        grad_fn=<PermuteBackward>),\n tensor([[[ 0.0209, -0.0165, -0.0397,  ..., -0.0246,  0.0174,  0.0609],\n          [ 0.0249, -0.0083, -0.0477,  ..., -0.0099, -0.0007,  0.0616],\n          [ 0.0080,  0.0013, -0.0091,  ..., -0.0068, -0.0088,  0.0575],\n          ...,\n          [ 0.0231,  0.0008, -0.0044,  ...,  0.0006,  0.0093,  0.0378],\n          [-0.0139, -0.0270, -0.0169,  ...,  0.0077,  0.0191,  0.0753],\n          [-0.0184, -0.0173, -0.0269,  ..., -0.0127,  0.0236,  0.0686]],\n \n         [[-0.0007, -0.0070, -0.0474,  ..., -0.0025,  0.0065,  0.0762],\n          [ 0.0073, -0.0155, -0.0391,  ..., -0.0045,  0.0090,  0.0474],\n          [ 0.0050,  0.0084, -0.0359,  ..., -0.0105, -0.0175,  0.0707],\n          ...,\n          [-0.0064, -0.0193, -0.0257,  ..., -0.0038,  0.0071,  0.0397],\n          [ 0.0047, -0.0181, -0.0297,  ..., -0.0125,  0.0060,  0.0668],\n          [ 0.0232,  0.0018, -0.0265,  ..., -0.0081,  0.0114,  0.0767]],\n \n         [[-0.0078, -0.0026, -0.0415,  ..., -0.0110,  0.0317,  0.0741],\n          [ 0.0302, -0.0138, -0.0337,  ..., -0.0329,  0.0164,  0.0684],\n          [ 0.0124, -0.0063, -0.0366,  ..., -0.0090,  0.0157,  0.0739],\n          ...,\n          [ 0.0089, -0.0183, -0.0191,  ..., -0.0085,  0.0351,  0.0782],\n          [-0.0130, -0.0145, -0.0399,  ...,  0.0009, -0.0009,  0.0750],\n          [-0.0070, -0.0155, -0.0428,  ...,  0.0071, -0.0004,  0.0852]],\n \n         ...,\n \n         [[ 0.0130, -0.0080, -0.0565,  ..., -0.0209,  0.0048,  0.0885],\n          [ 0.0098, -0.0282, -0.0362,  ..., -0.0254, -0.0165,  0.0623],\n          [ 0.0282, -0.0040, -0.0366,  ...,  0.0051, -0.0007,  0.0724],\n          ...,\n          [-0.0203, -0.0061, -0.0083,  ..., -0.0001,  0.0169,  0.0828],\n          [-0.0129, -0.0288, -0.0542,  ..., -0.0003,  0.0119,  0.0616],\n          [ 0.0097, -0.0037, -0.0137,  ..., -0.0309, -0.0095,  0.0592]],\n \n         [[-0.0013, -0.0335, -0.0383,  ..., -0.0037,  0.0096,  0.0807],\n          [ 0.0190, -0.0171, -0.0526,  ..., -0.0196,  0.0070,  0.0796],\n          [ 0.0129, -0.0316, -0.0490,  ...,  0.0052,  0.0070,  0.0833],\n          ...,\n          [-0.0010, -0.0102, -0.0230,  ..., -0.0225,  0.0059,  0.0873],\n          [ 0.0004, -0.0295, -0.0293,  ..., -0.0049, -0.0097,  0.0450],\n          [ 0.0090, -0.0162, -0.0325,  ..., -0.0290,  0.0168,  0.0799]],\n \n         [[ 0.0034, -0.0408, -0.0332,  ..., -0.0191,  0.0168,  0.0698],\n          [ 0.0076, -0.0103, -0.0173,  ..., -0.0135,  0.0031,  0.0638],\n          [-0.0084, -0.0112, -0.0150,  ...,  0.0010,  0.0232,  0.0829],\n          ...,\n          [ 0.0107, -0.0061, -0.0290,  ..., -0.0150,  0.0325,  0.0531],\n          [-0.0016, -0.0211, -0.0349,  ..., -0.0131,  0.0165,  0.0459],\n          [-0.0001, -0.0159, -0.0280,  ..., -0.0167,  0.0229,  0.0734]]],\n        device='cuda:0', grad_fn=<MishBackward>))"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-08-18T21:54:40.591122Z","iopub.execute_input":"2021-08-18T21:54:40.591457Z","iopub.status.idle":"2021-08-18T21:54:40.595934Z","shell.execute_reply.started":"2021-08-18T21:54:40.591428Z","shell.execute_reply":"2021-08-18T21:54:40.595024Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# For kaggle directory\n#import os \n#os.chdir(\"../input/neuralmanifoldanimals\")\nfrom sam import SAM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_optimizer = torch.optim.Adam  # define an optimizer for the \"sharpness-aware\" update\noptimizer = SAM(net.parameters(), base_optimizer, lr=0.000974, weight_decay = 0.00001)#, momentum=0.9)\n\n# you can keep re-running this cell if you think the cost might decrease further\n\ncost = nn.MSELoss()\n\ntrain_save = []\nvalid_save = []\n\nniter = 50000 #+30000 # \n# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\nfor k in tqdm(range(niter)):\n    net.train()\n    # the network outputs the single-neuron prediction and the latents\n    z, y = net(x1_train)\n\n    # our cost\n    loss = cost(z, x2_train)\n\n    # train the network as usual\n    loss.backward()\n    optimizer.first_step(zero_grad = True)\n    cost(net(x1_train)[0],x2_train).backward()\n    \n    optimizer.second_step(zero_grad=True)\n    \n    \"\"\" \"\"\";\n    # this can be run in a first phase, before wandb finetuning\n    with torch.no_grad():\n        net.eval()\n        train_save.append(loss.item())\n        valid_loss = cost(net(x1_valid)[0], x2_valid)\n        valid_save.append(valid_loss.item())\n   \n\n    if k % 50 == 0:\n        with torch.no_grad():\n            net.eval()\n            valid_loss = cost(net(x1_valid)[0], x2_valid)\n            \n            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')","metadata":{"id":"dve5X3J2NM2E","execution":{"iopub.status.busy":"2021-08-18T22:01:55.950188Z","iopub.execute_input":"2021-08-18T22:01:55.950510Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0487d3fcd4646feb260f3a2a2ceed41"}},"metadata":{}},{"name":"stdout","text":" iteration 0, train cost 3.5095, valid cost 3.6108\n iteration 50, train cost 2.4659, valid cost 2.5650\n iteration 100, train cost 2.4596, valid cost 2.5607\n iteration 150, train cost 2.4439, valid cost 2.5434\n iteration 200, train cost 2.2830, valid cost 2.3661\n iteration 250, train cost 2.1381, valid cost 2.2040\n iteration 300, train cost 2.0266, valid cost 2.0887\n iteration 350, train cost 1.9714, valid cost 2.0330\n iteration 400, train cost 1.9521, valid cost 2.0156\n iteration 450, train cost 1.9419, valid cost 2.0057\n iteration 500, train cost 1.9313, valid cost 1.9946\n iteration 550, train cost 1.9190, valid cost 1.9810\n iteration 600, train cost 1.9031, valid cost 1.9636\n iteration 650, train cost 1.8865, valid cost 1.9435\n iteration 700, train cost 1.8671, valid cost 1.9187\n iteration 750, train cost 1.8457, valid cost 1.8907\n iteration 800, train cost 1.8280, valid cost 1.8687\n iteration 850, train cost 1.8132, valid cost 1.8518\n iteration 900, train cost 1.7982, valid cost 1.8336\n iteration 950, train cost 1.7838, valid cost 1.8165\n iteration 1000, train cost 1.7716, valid cost 1.8017\n iteration 1050, train cost 1.7599, valid cost 1.7887\n iteration 1100, train cost 1.7488, valid cost 1.7763\n iteration 1150, train cost 1.7388, valid cost 1.7656\n iteration 1200, train cost 1.7307, valid cost 1.7566\n iteration 1250, train cost 1.7228, valid cost 1.7484\n iteration 1300, train cost 1.7181, valid cost 1.7433\n iteration 1350, train cost 1.7101, valid cost 1.7356\n iteration 1400, train cost 1.7027, valid cost 1.7280\n iteration 1450, train cost 1.6968, valid cost 1.7217\n iteration 1500, train cost 1.6906, valid cost 1.7157\n iteration 1550, train cost 1.6853, valid cost 1.7102\n iteration 1600, train cost 1.6800, valid cost 1.7044\n iteration 1650, train cost 1.6750, valid cost 1.6989\n iteration 1700, train cost 1.6699, valid cost 1.6932\n iteration 1750, train cost 1.6671, valid cost 1.6897\n iteration 1800, train cost 1.6609, valid cost 1.6828\n iteration 1850, train cost 1.6560, valid cost 1.6774\n iteration 1900, train cost 1.6513, valid cost 1.6724\n iteration 1950, train cost 1.6467, valid cost 1.6672\n iteration 2000, train cost 1.6420, valid cost 1.6621\n iteration 2050, train cost 1.6382, valid cost 1.6577\n iteration 2100, train cost 1.6345, valid cost 1.6534\n iteration 2150, train cost 1.6310, valid cost 1.6499\n iteration 2200, train cost 1.6282, valid cost 1.6468\n iteration 2250, train cost 1.6260, valid cost 1.6444\n iteration 2300, train cost 1.6237, valid cost 1.6419\n iteration 2350, train cost 1.6211, valid cost 1.6400\n iteration 2400, train cost 1.6193, valid cost 1.6380\n iteration 2450, train cost 1.6174, valid cost 1.6360\n iteration 2500, train cost 1.6158, valid cost 1.6342\n iteration 2550, train cost 1.6136, valid cost 1.6328\n iteration 2600, train cost 1.6126, valid cost 1.6307\n iteration 2650, train cost 1.6107, valid cost 1.6287\n iteration 2700, train cost 1.6087, valid cost 1.6268\n iteration 2750, train cost 1.6073, valid cost 1.6249\n iteration 2800, train cost 1.6051, valid cost 1.6234\n iteration 2850, train cost 1.6064, valid cost 1.6235\n iteration 2900, train cost 1.6025, valid cost 1.6207\n iteration 2950, train cost 1.6010, valid cost 1.6193\n iteration 3000, train cost 1.6000, valid cost 1.6182\n iteration 3050, train cost 1.5985, valid cost 1.6171\n iteration 3100, train cost 1.5973, valid cost 1.6161\n iteration 3150, train cost 1.5963, valid cost 1.6154\n iteration 3200, train cost 1.5951, valid cost 1.6143\n iteration 3250, train cost 1.5940, valid cost 1.6133\n iteration 3300, train cost 1.5926, valid cost 1.6122\n iteration 3350, train cost 1.5917, valid cost 1.6114\n iteration 3400, train cost 1.5904, valid cost 1.6105\n iteration 3450, train cost 1.5899, valid cost 1.6097\n iteration 3500, train cost 1.5890, valid cost 1.6090\n iteration 3550, train cost 1.5880, valid cost 1.6081\n iteration 3600, train cost 1.5869, valid cost 1.6075\n iteration 3650, train cost 1.5864, valid cost 1.6070\n iteration 3700, train cost 1.5855, valid cost 1.6066\n iteration 3750, train cost 1.5851, valid cost 1.6058\n iteration 3800, train cost 1.5843, valid cost 1.6052\n iteration 3850, train cost 1.5837, valid cost 1.6047\n iteration 3900, train cost 1.5826, valid cost 1.6040\n iteration 3950, train cost 1.5822, valid cost 1.6038\n iteration 4000, train cost 1.5814, valid cost 1.6032\n iteration 4050, train cost 1.5807, valid cost 1.6025\n iteration 4100, train cost 1.5802, valid cost 1.6019\n iteration 4150, train cost 1.5793, valid cost 1.6011\n iteration 4200, train cost 1.5783, valid cost 1.6003\n iteration 4250, train cost 1.5776, valid cost 1.5996\n iteration 4300, train cost 1.5768, valid cost 1.5987\n iteration 4350, train cost 1.5759, valid cost 1.5980\n iteration 4400, train cost 1.5751, valid cost 1.5975\n iteration 4450, train cost 1.5744, valid cost 1.5968\n iteration 4500, train cost 1.5737, valid cost 1.5963\n iteration 4550, train cost 1.5733, valid cost 1.5957\n iteration 4600, train cost 1.5728, valid cost 1.5950\n iteration 4650, train cost 1.5720, valid cost 1.5950\n iteration 4700, train cost 1.5717, valid cost 1.5944\n iteration 4750, train cost 1.5714, valid cost 1.5939\n iteration 4800, train cost 1.5709, valid cost 1.5936\n iteration 4850, train cost 1.5705, valid cost 1.5933\n iteration 4900, train cost 1.5700, valid cost 1.5929\n iteration 4950, train cost 1.5697, valid cost 1.5926\n iteration 5000, train cost 1.5695, valid cost 1.5923\n iteration 5050, train cost 1.5690, valid cost 1.5922\n iteration 5100, train cost 1.5686, valid cost 1.5917\n iteration 5150, train cost 1.5680, valid cost 1.5913\n iteration 5200, train cost 1.5682, valid cost 1.5910\n iteration 5250, train cost 1.5674, valid cost 1.5909\n iteration 5300, train cost 1.5668, valid cost 1.5907\n iteration 5350, train cost 1.5666, valid cost 1.5905\n iteration 5400, train cost 1.5660, valid cost 1.5901\n iteration 5450, train cost 1.5660, valid cost 1.5899\n iteration 5500, train cost 1.5659, valid cost 1.5895\n iteration 5550, train cost 1.5655, valid cost 1.5894\n iteration 5600, train cost 1.5653, valid cost 1.5893\n iteration 5650, train cost 1.5649, valid cost 1.5891\n iteration 5700, train cost 1.5644, valid cost 1.5887\n iteration 5750, train cost 1.5643, valid cost 1.5884\n iteration 5800, train cost 1.5639, valid cost 1.5882\n iteration 5850, train cost 1.5636, valid cost 1.5880\n iteration 5900, train cost 1.5633, valid cost 1.5879\n iteration 5950, train cost 1.5632, valid cost 1.5875\n iteration 6000, train cost 1.5630, valid cost 1.5873\n iteration 6050, train cost 1.5625, valid cost 1.5873\n iteration 6100, train cost 1.5623, valid cost 1.5869\n iteration 6150, train cost 1.5621, valid cost 1.5868\n iteration 6200, train cost 1.5618, valid cost 1.5865\n iteration 6250, train cost 1.5619, valid cost 1.5869\n iteration 6300, train cost 1.5615, valid cost 1.5862\n iteration 6350, train cost 1.5609, valid cost 1.5859\n iteration 6400, train cost 1.5606, valid cost 1.5858\n iteration 6450, train cost 1.5605, valid cost 1.5854\n iteration 6500, train cost 1.5601, valid cost 1.5853\n iteration 6550, train cost 1.5602, valid cost 1.5852\n iteration 6600, train cost 1.5601, valid cost 1.5851\n iteration 6650, train cost 1.5593, valid cost 1.5850\n iteration 6700, train cost 1.5587, valid cost 1.5847\n iteration 6750, train cost 1.5588, valid cost 1.5843\n iteration 6800, train cost 1.5587, valid cost 1.5844\n iteration 6850, train cost 1.5585, valid cost 1.5841\n iteration 6900, train cost 1.5582, valid cost 1.5838\n iteration 6950, train cost 1.5588, valid cost 1.5838\n iteration 7000, train cost 1.5575, valid cost 1.5834\n iteration 7050, train cost 1.5575, valid cost 1.5834\n iteration 7100, train cost 1.5573, valid cost 1.5832\n iteration 7150, train cost 1.5570, valid cost 1.5830\n iteration 7200, train cost 1.5570, valid cost 1.5828\n iteration 7250, train cost 1.5568, valid cost 1.5828\n iteration 7300, train cost 1.5563, valid cost 1.5825\n iteration 7350, train cost 1.5686, valid cost 1.5912\n iteration 7400, train cost 1.5614, valid cost 1.5857\n iteration 7450, train cost 1.5583, valid cost 1.5833\n iteration 7500, train cost 1.5573, valid cost 1.5828\n iteration 7550, train cost 1.5566, valid cost 1.5826\n iteration 7600, train cost 1.5563, valid cost 1.5822\n iteration 7650, train cost 1.5561, valid cost 1.5818\n iteration 7700, train cost 1.5558, valid cost 1.5816\n iteration 7750, train cost 1.5554, valid cost 1.5816\n iteration 7800, train cost 1.5551, valid cost 1.5812\n iteration 7850, train cost 1.5547, valid cost 1.5810\n iteration 7900, train cost 1.5544, valid cost 1.5808\n iteration 7950, train cost 1.5541, valid cost 1.5807\n iteration 8000, train cost 1.5540, valid cost 1.5806\n iteration 8050, train cost 1.5538, valid cost 1.5803\n iteration 8100, train cost 1.5536, valid cost 1.5800\n iteration 8150, train cost 1.5532, valid cost 1.5800\n iteration 8200, train cost 1.5527, valid cost 1.5799\n iteration 8250, train cost 1.5530, valid cost 1.5794\n iteration 8300, train cost 1.5526, valid cost 1.5794\n iteration 8350, train cost 1.5523, valid cost 1.5793\n iteration 8400, train cost 1.5523, valid cost 1.5791\n iteration 8450, train cost 1.5520, valid cost 1.5791\n iteration 8500, train cost 1.5517, valid cost 1.5788\n iteration 8550, train cost 1.5518, valid cost 1.5787\n iteration 8600, train cost 1.5512, valid cost 1.5785\n iteration 8650, train cost 1.5511, valid cost 1.5784\n iteration 8700, train cost 1.5509, valid cost 1.5782\n iteration 8750, train cost 1.5509, valid cost 1.5781\n iteration 8800, train cost 1.5505, valid cost 1.5779\n iteration 8850, train cost 1.5506, valid cost 1.5777\n iteration 8900, train cost 1.5504, valid cost 1.5777\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#import os \n#os.chdir(\"../../working\")","metadata":{"execution":{"iopub.status.busy":"2021-08-18T22:00:16.381598Z","iopub.execute_input":"2021-08-18T22:00:16.381985Z","iopub.status.idle":"2021-08-18T22:00:16.385757Z","shell.execute_reply.started":"2021-08-18T22:00:16.381949Z","shell.execute_reply":"2021-08-18T22:00:16.384932Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"torch.save(net.state_dict(), \"Net_Complete.pt\")","metadata":{"id":"5t4h3-K8NOtK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save(\"train_curve.npy\", np.asarray(train_save))\nnp.save(\"valid_curve.npy\", np.asarray(valid_save))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"while True:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the model back ðŸ˜‚ (it took one day to understand how to do it!)\n#model = Net(ncomp, NN1, NN2, bidi = True).to(device)\n#model.load_state_dict(torch.load(\"Net_Complete.pt\"))","metadata":{"id":"pVH1Zt_5WjLa","outputId":"2d016025-3f7c-43e6-b779-4af7bfde1531"},"execution_count":null,"outputs":[]}]}