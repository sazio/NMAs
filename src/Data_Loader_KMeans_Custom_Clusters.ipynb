{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji-aTo7SA1AB"
   },
   "source": [
    "# Exploratory Data Analysis of Stringer Dataset \n",
    "@authors: Simone Azeglio, Chetan Dhulipalla , Khalid Saifullah \n",
    "\n",
    "\n",
    "Part of the code here has been taken from [Neuromatch Academy's Computational Neuroscience Course](https://compneuro.neuromatch.io/projects/neurons/README.html), and specifically from [this notebook](https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/projects/neurons/load_stringer_spontaneous.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs7e5ppCMYCK"
   },
   "source": [
    "## Loading of Stringer spontaneous data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "u0nA90QhJurD"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "import os, requests\n",
    "\n",
    "fname = \"stringer_spontaneous.npy\"\n",
    "url = \"https://osf.io/dpqaj/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "FgbdwXWDSUpO"
   },
   "outputs": [],
   "source": [
    "#@title Import matplotlib and set defaults\n",
    "from matplotlib import rcParams \n",
    "from matplotlib import pyplot as plt\n",
    "rcParams['figure.figsize'] = [20, 4]\n",
    "rcParams['font.size'] =15\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRWWoEX0-sYp"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ulJ34TyRZo6",
    "outputId": "df56f72e-e25f-4702-b021-bf6959e018c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sresp', 'run', 'beh_svd_time', 'beh_svd_mask', 'stat', 'pupilArea', 'pupilCOM', 'xyz'])\n"
     ]
    }
   ],
   "source": [
    "#@title Data loading\n",
    "import numpy as np\n",
    "dat = np.load('stringer_spontaneous.npy', allow_pickle=True).item()\n",
    "print(dat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KGn2iJGmFpLC"
   },
   "outputs": [],
   "source": [
    "# functions \n",
    "\n",
    "def moving_avg(array, factor = 5):\n",
    "    \"\"\"Reducing the number of compontents by averaging of N = factor\n",
    "    subsequent elements of array\"\"\"\n",
    "    #zeros_ = np.zeros((array.shape[0], 2))\n",
    "    #array = np.hstack((array, zeros_))\n",
    "\n",
    "    array = np.reshape(array, (array.shape[0],  int(array.shape[1]/factor), factor))\n",
    "    array = np.mean(array, axis = 2)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdjYTZeV-yhR"
   },
   "source": [
    "## Spatial (XYZ) KMeans Clustering on Neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd \n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mEy_qiyKY1xG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#-------- Number of clusters = 2 ---------#\n",
      "\n",
      "AVG Score = 0.8026391622584995\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 3 ---------#\n",
      "\n",
      "AVG Score = 0.774292267867819\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 4 ---------#\n",
      "\n",
      "AVG Score = 0.7595856153841039\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 5 ---------#\n",
      "\n",
      "AVG Score = 0.7454677711233557\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 6 ---------#\n",
      "\n",
      "AVG Score = 0.729823934048344\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 7 ---------#\n",
      "\n",
      "AVG Score = 0.720037301802729\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 8 ---------#\n",
      "\n",
      "AVG Score = 0.7088019355781993\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 9 ---------#\n",
      "\n",
      "AVG Score = 0.7041424820490833\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 10 ---------#\n",
      "\n",
      "AVG Score = 0.6913856226841615\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 11 ---------#\n",
      "\n",
      "AVG Score = 0.6968938792641002\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 12 ---------#\n",
      "\n",
      "AVG Score = 0.6849206700764996\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 13 ---------#\n",
      "\n",
      "AVG Score = 0.6928820395432388\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 14 ---------#\n",
      "\n",
      "AVG Score = 0.6833208401279826\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 15 ---------#\n",
      "\n",
      "AVG Score = 0.6826468588664822\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 16 ---------#\n",
      "\n",
      "AVG Score = 0.6642014992544555\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 17 ---------#\n",
      "\n",
      "AVG Score = 0.6639670008789514\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 18 ---------#\n",
      "\n",
      "AVG Score = 0.6666566410545783\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 19 ---------#\n",
      "\n",
      "AVG Score = 0.6626954732513849\n",
      "\n",
      "#-------------------------------#\n",
      "\n",
      "#-------- Number of clusters = 20 ---------#\n",
      "\n",
      "AVG Score = 0.6500411881044835\n",
      "\n",
      "#-------------------------------#\n"
     ]
    }
   ],
   "source": [
    "for n_clusters in range(2, 21):\n",
    "\n",
    "    kmeans = KMeans(n_clusters= n_clusters, random_state=2021).fit(dat[\"xyz\"].T)\n",
    "\n",
    "    labels = kmeans.labels_#le.fit_transform(z)\n",
    "    ### least represented class (layer with less neurons)\n",
    "    n_samples = np.min(np.histogram(labels, bins= n_clusters)[0])\n",
    "    #print(n_samples)\n",
    "\n",
    "    dataSet = pd.DataFrame(dat[\"sresp\"])\n",
    "    dataSet[\"label\"] = labels \n",
    "\n",
    "    # it can be done in one loop ... \n",
    "    data_ = []\n",
    "    for i in range(0, n_clusters):\n",
    "        data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n",
    "\n",
    "    dataRNN = np.zeros((n_samples*n_clusters, dataSet.shape[1]-1))\n",
    "    for i in range(0,n_clusters):\n",
    "\n",
    "        # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n",
    "        ## normalized by layer\n",
    "        dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n",
    "\n",
    "    # downsampling and averaging \n",
    "    #avgd_normed_dataRNN = dataRNN#\n",
    "    avgd_normed_dataRNN = moving_avg(dataRNN, factor=2)\n",
    "\n",
    "    #-----------------------------#\n",
    "    print(\"\\n#-------- Number of clusters = \" + str(n_clusters) +\" ---------#\")\n",
    "    saved_pca = []\n",
    "    for mul in range(0,n_clusters):\n",
    "        pca = PCA(n_components=n_samples//10 + 1, random_state = 2021)\n",
    "        pca.fit(avgd_normed_dataRNN[n_samples*mul:n_samples*(mul+1),:])\n",
    "        saved_pca.append(np.cumsum(pca.explained_variance_ratio_)[n_samples//10])\n",
    "        #print(np.cumsum(pca.explained_variance_ratio_)[100])\n",
    "    print(\"\\nAVG Score = \" + str(sum(saved_pca)/len(saved_pca)))\n",
    "    print(\"\\n#-------------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Slicing Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#-------- Number of clusters = 9 ---------#\n",
      "\n",
      "AVG Score = 0.7352370554189993\n",
      "\n",
      "#-------------------------------#\n"
     ]
    }
   ],
   "source": [
    "# Extract labels from z - coordinate\n",
    "x, y, z = dat['xyz']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(z)\n",
    "### least represented class (layer with less neurons)\n",
    "n_samples = np.min(np.histogram(labels, bins=9)[0])\n",
    "\n",
    "\n",
    "dataSet = pd.DataFrame(dat[\"sresp\"])\n",
    "dataSet[\"label\"] = labels \n",
    "\n",
    "data_ = []\n",
    "for i in range(0, 9):\n",
    "    data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n",
    "\n",
    "dataRNN = np.zeros((n_samples*9, dataSet.shape[1]-1))\n",
    "for i in range(0,9):\n",
    "    \n",
    "    # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n",
    "    ## normalized by layer\n",
    "    dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n",
    "    \n",
    "avgd_normed_dataRNN = moving_avg(dataRNN, factor=2)\n",
    "\n",
    "print(\"\\n#-------- Number of clusters = \" + str(9) +\" ---------#\")\n",
    "saved_pca = []\n",
    "for mul in range(0,9):\n",
    "    pca = PCA(n_components=n_samples//10 + 1, random_state = 2021)\n",
    "    pca.fit(avgd_normed_dataRNN[n_samples*mul:n_samples*(mul+1),:])\n",
    "    saved_pca.append(np.cumsum(pca.explained_variance_ratio_)[n_samples//10])\n",
    "    #print(np.cumsum(pca.explained_variance_ratio_)[100])\n",
    "print(\"\\nAVG Score = \" + str(sum(saved_pca)/len(saved_pca)))\n",
    "print(\"\\n#-------------------------------#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can be done in one loop ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MrXC5QIiyhJ"
   },
   "source": [
    "## Data Loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "255tz5iqmSq1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "467WOHhtmXmb"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Dcb5vW2joVW_"
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# number of neurons \n",
    "NN = dataRNN.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXJPQdu17Ns5",
    "outputId": "18b700ff-0126-458f-a300-7488e457013f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7840, 3509)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# swapping the axes to maintain consistency with seq2seq notebook in the following code - the network takes all the neurons at a time step as input, not just one neuron\n",
    "\n",
    "# avgd_normed_dataRNN = np.swapaxes(avgd_normed_dataRNN, 0, 1)\n",
    "avgd_normed_dataRNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prl0OxLZkka9"
   },
   "outputs": [],
   "source": [
    "frac = 4/5\n",
    "\n",
    "#x1 = torch.from_numpy(dataRNN[:,:int(frac*dataRNN.shape[1])]).to(device).float().unsqueeze(0)\n",
    "#x2 = torch.from_numpy(dataRNN[:,int(frac*dataRNN.shape[1]):]).to(device).float().unsqueeze(0)\n",
    "#x1 = torch.from_numpy(avgd_normed_dataRNN[:1131,:]).to(device).float().unsqueeze(2)\n",
    "#x2 = torch.from_numpy(avgd_normed_dataRNN[:1131,:]).to(device).float().unsqueeze(2)\n",
    "\n",
    "n_neurs = 6723//9\n",
    "# let's use n_neurs/10 latent components\n",
    "ncomp = int(n_neurs/10)\n",
    "\n",
    "x1_train = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,:int(frac*avgd_normed_dataRNN.shape[1])]).to(device).float().unsqueeze(2)\n",
    "x2_train = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,:int(frac*avgd_normed_dataRNN.shape[1])]).to(device).float().unsqueeze(2)\n",
    "\n",
    "x1_valid = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,int(frac*avgd_normed_dataRNN.shape[1]):]).to(device).float().unsqueeze(2)\n",
    "x2_valid = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,int(frac*avgd_normed_dataRNN.shape[1]):]).to(device).float().unsqueeze(2)\n",
    "\n",
    "NN1 = x1_train.shape[0]\n",
    "NN2 = x2_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01IvhjPzk-Jw"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ncomp, NN1, NN2, bidi=True):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # play with some of the options in the RNN!\n",
    "        \n",
    "        self.rnn = nn.LSTM(NN1, ncomp, num_layers = 1, dropout = 0.,\n",
    "                         bidirectional = bidi)\n",
    "        \"\"\"\n",
    "        self.rnn = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0,\n",
    "                    bidirectional = bidi, nonlinearity = 'tanh')\n",
    "        self.rnn = nn.GRU(NN1, ncomp, num_layers = 1, dropout = 0,\n",
    "                         bidirectional = bidi)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "                    nn.Linear(ncomp, ncomp*5),\n",
    "                    nn.Mish(),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(ncomp*5, ncomp*5),\n",
    "                    nn.Mish(),\n",
    "                    nn.Dropout(),\n",
    "                    nn.Linear(ncomp*5, ncomp), \n",
    "                    nn.Mish())\n",
    "        \n",
    "        self.fc = nn.Linear(ncomp, NN2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 2, 0)\n",
    "        #print(x.shape)\n",
    "        # h_0 = torch.zeros(2, x.size()[1], self.ncomp).to(device)\n",
    "        \n",
    "        y, h_n = self.rnn(x)\n",
    "\n",
    "        #print(y.shape)\n",
    "        #print(h_n.shape)\n",
    "        if self.rnn.bidirectional:\n",
    "          # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n",
    "          # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n",
    "            q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n",
    "        else:\n",
    "            q = y\n",
    "        \n",
    "        q = self.mlp(q)\n",
    "\n",
    "        # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n",
    "        # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n",
    "        #z = F.softplus(self.fc(q), 10)\n",
    "        #print(q.shape)\n",
    "        z = self.fc(q).permute(2, 0, 1)\n",
    "        # print(z.shape)\n",
    "        return z, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQGEVQaGmwV6"
   },
   "outputs": [],
   "source": [
    "# we initialize the neural network\n",
    "net = Net(ncomp, NN1, NN2, bidi = True).to(device)\n",
    "\n",
    "# special thing:  we initialize the biases of the last layer in the neural network\n",
    "# we set them as the mean firing rates of the neurons.\n",
    "# this should make the initial predictions close to the mean, because the latents don't contribute much\n",
    "net.fc.bias.data[:] = x1_train.mean(axis = (0,1))\n",
    "\n",
    "# we set up the optimizer. Adjust the learning rate if the training is slow or if it explodes.\n",
    "optimizer1 = torch.optim.Adam(net.parameters(), lr=.0001)\n",
    "# optimizer2 = torch.optim.SGD(net.parameters(), lr = 0.0001, momentum = 0.9, weight_decay = 0.01, )\n",
    "# optimizer3 = torch.optim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzvKjuTvmwYa",
    "outputId": "63095320-91b6-4d1b-9e77-7332360a7c86"
   },
   "outputs": [],
   "source": [
    "# forward check \n",
    "# net(x1)\n",
    "net(x1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hlfx8Ltp5jJ"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjzD06B9ta3n"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5REhdgsHZF2",
    "outputId": "6d495e15-4aa2-47fd-c362-00561efe4f9a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sam import SAM\n",
    "\n",
    "base_optimizer = torch.optim.Adam  # define an optimizer for the \"sharpness-aware\" update\n",
    "optimizer = SAM(net.parameters(), base_optimizer, lr=0.001, weight_decay = 1e-5)#, momentum=0.9)\n",
    "\n",
    "# you can keep re-running this cell if you think the cost might decrease further\n",
    "\n",
    "cost = nn.MSELoss()\n",
    "\n",
    "#train_save = []\n",
    "#valid_save = []\n",
    "\n",
    "niter =  5500 # \n",
    "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\n",
    "for k in tqdm(range(niter)):\n",
    "    net.train()\n",
    "    # the network outputs the single-neuron prediction and the latents\n",
    "    z, y = net(x1_train)\n",
    "\n",
    "    # our cost\n",
    "    loss = cost(z, x2_train)\n",
    "\n",
    "    # train the network as usual\n",
    "    loss.backward()\n",
    "    optimizer.first_step(zero_grad = True)\n",
    "    \n",
    "    cost(net(x1_train)[0],x2_train).backward()\n",
    "    \n",
    "    optimizer.second_step(zero_grad=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        train_save.append(loss.item())\n",
    "        valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
    "        valid_save.append(valid_loss.item())\n",
    "\n",
    "    if k % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
    "            \n",
    "            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW4j2gM2HZF2"
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_save)\n",
    "plt.plot(loss_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24uTE21zHZF2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNG8yY9fp6sg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Original training\n",
    "# you can keep re-running this cell if you think the cost might decrease further\n",
    "\n",
    "cost = nn.MSELoss()\n",
    "\n",
    "niter = 5800\n",
    "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\n",
    "for k in tqdm(range(niter)):\n",
    "    net.train()\n",
    "    # the network outputs the single-neuron prediction and the latents\n",
    "    z, y = net(x1_train)\n",
    "\n",
    "    # our cost\n",
    "    loss = cost(z, x2_train)\n",
    "\n",
    "    # train the network as usual\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer1.zero_grad()\n",
    "    \n",
    "\n",
    "    if k % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
    "            \n",
    "            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')\n",
    "\n",
    "\"\"\";  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkvwOHwjHZF2"
   },
   "source": [
    "## Validation from same neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jryOMKdDJoEh"
   },
   "outputs": [],
   "source": [
    "test, hidden = net(x1_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HY_xlS8k7BF8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x2_valid[7,:,0].cpu().detach().numpy())\n",
    "plt.plot(test[7,:,0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emfEWsfDHZF3"
   },
   "source": [
    "## Testing neurons from same layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJ0dBUgIHZF3"
   },
   "outputs": [],
   "source": [
    "test, hidden = net(torch.from_numpy(avgd_normed_dataRNN[n_neurs:2*n_neurs,:]).unsqueeze(2).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RORJkXXOHZF3"
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZPVIPiOHZF3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_n = 15\n",
    "plt.plot(test[n_n,:,0].cpu().detach().numpy())\n",
    "plt.plot(avgd_normed_dataRNN[n_neurs + n_n,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot1-cMeeHZF4"
   },
   "source": [
    "## Testing neurons from another layer (#9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swaUYZLeHZF4"
   },
   "outputs": [],
   "source": [
    "test, hidden = net(torch.from_numpy(avgd_normed_dataRNN[10000:10100,:]).unsqueeze(2).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuuFFupaHZF4"
   },
   "outputs": [],
   "source": [
    "plt.plot(test[10,:,0].cpu().detach().numpy())\n",
    "plt.plot(avgd_normed_dataRNN[10010,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI-JJAbQHZF4"
   },
   "source": [
    "# Training 9 Networks \n",
    "Each Network corresponds to a different layer in V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0teh-dlHZF4"
   },
   "outputs": [],
   "source": [
    "# you can keep re-running this cell if you think the cost might decrease further\n",
    "\n",
    "cost = nn.MSELoss()\n",
    "\n",
    "niter = 10000\n",
    "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\n",
    "for k in tqdm(range(niter)):\n",
    "    net.train()\n",
    "    # the network outputs the single-neuron prediction and the latents\n",
    "    z, y = net(x1_train)\n",
    "\n",
    "    # our cost\n",
    "    loss = cost(z, x2_train)\n",
    "\n",
    "    # train the network as usual\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    optimizer1.zero_grad()\n",
    "\n",
    "    if k % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
    "            \n",
    "            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kszTdz6bHZF4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Loader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
