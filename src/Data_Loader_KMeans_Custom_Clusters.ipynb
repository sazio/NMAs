{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data_Loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sazio/NMAs/blob/main/src/Data_Loader_KMeans_Custom_Clusters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji-aTo7SA1AB"
      },
      "source": [
        "# Exploratory Data Analysis of Stringer Dataset \n",
        "@authors: Simone Azeglio, Chetan Dhulipalla , Khalid Saifullah \n",
        "\n",
        "\n",
        "Part of the code here has been taken from [Neuromatch Academy's Computational Neuroscience Course](https://compneuro.neuromatch.io/projects/neurons/README.html), and specifically from [this notebook](https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/projects/neurons/load_stringer_spontaneous.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs7e5ppCMYCK"
      },
      "source": [
        "## Loading of Stringer spontaneous data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "u0nA90QhJurD"
      },
      "source": [
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = \"stringer_spontaneous.npy\"\n",
        "url = \"https://osf.io/dpqaj/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "    except requests.ConnectionError:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "        if r.status_code != requests.codes.ok:\n",
        "            print(\"!!! Failed to download data !!!\")\n",
        "        else:\n",
        "            with open(fname, \"wb\") as fid:\n",
        "                fid.write(r.content)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FgbdwXWDSUpO"
      },
      "source": [
        "#@title Import matplotlib and set defaults\n",
        "from matplotlib import rcParams \n",
        "from matplotlib import pyplot as plt\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] =15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWWoEX0-sYp"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ulJ34TyRZo6",
        "outputId": "af79ed9d-f763-4333-b7d4-ddbc5dcc2e5e"
      },
      "source": [
        "#@title Data loading\n",
        "import numpy as np\n",
        "dat = np.load('stringer_spontaneous.npy', allow_pickle=True).item()\n",
        "print(dat.keys())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['sresp', 'run', 'beh_svd_time', 'beh_svd_mask', 'stat', 'pupilArea', 'pupilCOM', 'xyz'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGn2iJGmFpLC"
      },
      "source": [
        "# functions \n",
        "\n",
        "def moving_avg(array, factor = 5):\n",
        "    \"\"\"Reducing the number of compontents by averaging of N = factor\n",
        "    subsequent elements of array\"\"\"\n",
        "    #zeros_ = np.zeros((array.shape[0], 2))\n",
        "    #array = np.hstack((array, zeros_))\n",
        "\n",
        "    array = np.reshape(array, (array.shape[0],  int(array.shape[1]/factor), factor))\n",
        "    array = np.mean(array, axis = 2)\n",
        "\n",
        "    return array"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdjYTZeV-yhR"
      },
      "source": [
        "## Spatial (XYZ) KMeans Clustering on Neurons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ip_Y_4EXm7W"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd \n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEy_qiyKY1xG",
        "outputId": "cf3b737d-9177-469e-dbe9-6a705c6a7bb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for n_clusters in range(2, 21):\n",
        "\n",
        "    kmeans = KMeans(n_clusters= n_clusters, random_state=2021).fit(dat[\"xyz\"].T)\n",
        "\n",
        "    labels = kmeans.labels_#le.fit_transform(z)\n",
        "    ### least represented class (layer with less neurons)\n",
        "    n_samples = np.min(np.histogram(labels, bins= n_clusters)[0])\n",
        "    #print(n_samples)\n",
        "\n",
        "    dataSet = pd.DataFrame(dat[\"sresp\"])\n",
        "    dataSet[\"label\"] = labels \n",
        "\n",
        "    # it can be done in one loop ... \n",
        "    data_ = []\n",
        "    for i in range(0, n_clusters):\n",
        "        data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n",
        "\n",
        "    dataRNN = np.zeros((n_samples*n_clusters, dataSet.shape[1]-1))\n",
        "    for i in range(0,n_clusters):\n",
        "\n",
        "        # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n",
        "        ## normalized by layer\n",
        "        dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n",
        "\n",
        "    # downsampling and averaging \n",
        "    #avgd_normed_dataRNN = dataRNN#\n",
        "    avgd_normed_dataRNN = moving_avg(dataRNN, factor=2)\n",
        "\n",
        "    #-----------------------------#\n",
        "    print(\"\\n#-------- Number of clusters = \" + str(n_clusters) +\" ---------#\")\n",
        "    print(\"\\nNumber of samples for each layer = \" + str(n_samples) +\" \\tFraction of samples = \" + str(n_samples//10))\n",
        "    saved_pca = []\n",
        "    for mul in range(0,n_clusters):\n",
        "        pca = PCA(n_components=n_samples//10 + 1, random_state = 2021)\n",
        "        pca.fit(avgd_normed_dataRNN[n_samples*mul:n_samples*(mul+1),:])\n",
        "        saved_pca.append(np.cumsum(pca.explained_variance_ratio_)[n_samples//10])\n",
        "        #print(np.cumsum(pca.explained_variance_ratio_)[100])\n",
        "    print(\"\\nAVG Score = \" + str(sum(saved_pca)/len(saved_pca)))\n",
        "    print(\"\\n#-------------------------------#\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#-------- Number of clusters = 2 ---------#\n",
            "\n",
            "Number of samples for each layer = 5076 \tFraction of samples = 507\n",
            "\n",
            "AVG Score = 0.801289593632871\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 3 ---------#\n",
            "\n",
            "Number of samples for each layer = 3369 \tFraction of samples = 336\n",
            "\n",
            "AVG Score = 0.7729962821881715\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 4 ---------#\n",
            "\n",
            "Number of samples for each layer = 2428 \tFraction of samples = 242\n",
            "\n",
            "AVG Score = 0.760049991809201\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 5 ---------#\n",
            "\n",
            "Number of samples for each layer = 1621 \tFraction of samples = 162\n",
            "\n",
            "AVG Score = 0.7403165040256787\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 6 ---------#\n",
            "\n",
            "Number of samples for each layer = 1503 \tFraction of samples = 150\n",
            "\n",
            "AVG Score = 0.7247217785790309\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 7 ---------#\n",
            "\n",
            "Number of samples for each layer = 1358 \tFraction of samples = 135\n",
            "\n",
            "AVG Score = 0.7248814153692871\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 8 ---------#\n",
            "\n",
            "Number of samples for each layer = 1023 \tFraction of samples = 102\n",
            "\n",
            "AVG Score = 0.7076095485650018\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 9 ---------#\n",
            "\n",
            "Number of samples for each layer = 714 \tFraction of samples = 71\n",
            "\n",
            "AVG Score = 0.7010629495685896\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 10 ---------#\n",
            "\n",
            "Number of samples for each layer = 700 \tFraction of samples = 70\n",
            "\n",
            "AVG Score = 0.685033437304012\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 11 ---------#\n",
            "\n",
            "Number of samples for each layer = 675 \tFraction of samples = 67\n",
            "\n",
            "AVG Score = 0.6884380938552874\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 12 ---------#\n",
            "\n",
            "Number of samples for each layer = 537 \tFraction of samples = 53\n",
            "\n",
            "AVG Score = 0.6788022400526864\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 13 ---------#\n",
            "\n",
            "Number of samples for each layer = 513 \tFraction of samples = 51\n",
            "\n",
            "AVG Score = 0.6798266093560557\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 14 ---------#\n",
            "\n",
            "Number of samples for each layer = 519 \tFraction of samples = 51\n",
            "\n",
            "AVG Score = 0.6816340061906258\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 15 ---------#\n",
            "\n",
            "Number of samples for each layer = 509 \tFraction of samples = 50\n",
            "\n",
            "AVG Score = 0.6743775139872691\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 16 ---------#\n",
            "\n",
            "Number of samples for each layer = 475 \tFraction of samples = 47\n",
            "\n",
            "AVG Score = 0.6702459679814924\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 17 ---------#\n",
            "\n",
            "Number of samples for each layer = 426 \tFraction of samples = 42\n",
            "\n",
            "AVG Score = 0.657628719063806\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 18 ---------#\n",
            "\n",
            "Number of samples for each layer = 457 \tFraction of samples = 45\n",
            "\n",
            "AVG Score = 0.6664942227690704\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 19 ---------#\n",
            "\n",
            "Number of samples for each layer = 456 \tFraction of samples = 45\n",
            "\n",
            "AVG Score = 0.6598017204295704\n",
            "\n",
            "#-------------------------------#\n",
            "\n",
            "#-------- Number of clusters = 20 ---------#\n",
            "\n",
            "Number of samples for each layer = 441 \tFraction of samples = 44\n",
            "\n",
            "AVG Score = 0.6602377320309891\n",
            "\n",
            "#-------------------------------#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vENGJihcXm7X"
      },
      "source": [
        "## Z-Slicing Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iku9RtPXm7X"
      },
      "source": [
        "import pandas as pd \n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg5YsYOlXm7X",
        "outputId": "ecd82856-a36e-46a4-e111-3160011d2267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Extract labels from z - coordinate\n",
        "x, y, z = dat['xyz']\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "labels = le.fit_transform(z)\n",
        "### least represented class (layer with less neurons)\n",
        "n_samples = np.min(np.histogram(labels, bins=9)[0])\n",
        "\n",
        "\n",
        "dataSet = pd.DataFrame(dat[\"sresp\"])\n",
        "dataSet[\"label\"] = labels \n",
        "\n",
        "data_ = []\n",
        "for i in range(0, 9):\n",
        "    data_.append(dataSet[dataSet[\"label\"] == i].sample(n = n_samples).iloc[:,:-1].values)\n",
        "\n",
        "dataRNN = np.zeros((n_samples*9, dataSet.shape[1]-1))\n",
        "for i in range(0,9):\n",
        "    \n",
        "    # dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]\n",
        "    ## normalized by layer\n",
        "    dataRNN[n_samples*i:n_samples*(i+1), :] = data_[i]/np.mean(np.asarray(data_)[i,:,:], axis = 0)\n",
        "    \n",
        "avgd_normed_dataRNN = moving_avg(dataRNN, factor=2)\n",
        "\n",
        "print(\"\\n#-------- Number of clusters = \" + str(9) +\" ---------#\")\n",
        "print(\"\\nNumber of samples for each layer = \" + str(n_samples) +\" \\tFraction of samples = \" + str(n_samples//10))\n",
        "saved_pca = []\n",
        "for mul in range(0,9):\n",
        "    pca = PCA(n_components=n_samples//10 + 1, random_state = 2021)\n",
        "    pca.fit(avgd_normed_dataRNN[n_samples*mul:n_samples*(mul+1),:])\n",
        "    saved_pca.append(np.cumsum(pca.explained_variance_ratio_)[n_samples//10])\n",
        "    #print(np.cumsum(pca.explained_variance_ratio_)[100])\n",
        "print(\"\\nAVG Score = \" + str(sum(saved_pca)/len(saved_pca)))\n",
        "print(\"\\n#-------------------------------#\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#-------- Number of clusters = 9 ---------#\n",
            "\n",
            "Number of samples for each layer = 1131 \tFraction of samples = 113\n",
            "\n",
            "AVG Score = 0.7334629611948738\n",
            "\n",
            "#-------------------------------#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op_7895XXm7Y"
      },
      "source": [
        "# it can be done in one loop ... \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MrXC5QIiyhJ"
      },
      "source": [
        "## Data Loader \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "255tz5iqmSq1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "467WOHhtmXmb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcb5vW2joVW_"
      },
      "source": [
        "# set the seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# number of neurons \n",
        "NN = dataRNN.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXJPQdu17Ns5",
        "outputId": "18b700ff-0126-458f-a300-7488e457013f"
      },
      "source": [
        "# swapping the axes to maintain consistency with seq2seq notebook in the following code - the network takes all the neurons at a time step as input, not just one neuron\n",
        "\n",
        "# avgd_normed_dataRNN = np.swapaxes(avgd_normed_dataRNN, 0, 1)\n",
        "avgd_normed_dataRNN.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7840, 3509)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prl0OxLZkka9"
      },
      "source": [
        "frac = 4/5\n",
        "\n",
        "#x1 = torch.from_numpy(dataRNN[:,:int(frac*dataRNN.shape[1])]).to(device).float().unsqueeze(0)\n",
        "#x2 = torch.from_numpy(dataRNN[:,int(frac*dataRNN.shape[1]):]).to(device).float().unsqueeze(0)\n",
        "#x1 = torch.from_numpy(avgd_normed_dataRNN[:1131,:]).to(device).float().unsqueeze(2)\n",
        "#x2 = torch.from_numpy(avgd_normed_dataRNN[:1131,:]).to(device).float().unsqueeze(2)\n",
        "\n",
        "n_neurs = 6723//9\n",
        "# let's use n_neurs/10 latent components\n",
        "ncomp = int(n_neurs/10)\n",
        "\n",
        "x1_train = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,:int(frac*avgd_normed_dataRNN.shape[1])]).to(device).float().unsqueeze(2)\n",
        "x2_train = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,:int(frac*avgd_normed_dataRNN.shape[1])]).to(device).float().unsqueeze(2)\n",
        "\n",
        "x1_valid = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,int(frac*avgd_normed_dataRNN.shape[1]):]).to(device).float().unsqueeze(2)\n",
        "x2_valid = torch.from_numpy(avgd_normed_dataRNN[:n_neurs,int(frac*avgd_normed_dataRNN.shape[1]):]).to(device).float().unsqueeze(2)\n",
        "\n",
        "NN1 = x1_train.shape[0]\n",
        "NN2 = x2_train.shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IvhjPzk-Jw"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ncomp, NN1, NN2, bidi=True):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # play with some of the options in the RNN!\n",
        "        \n",
        "        self.rnn = nn.LSTM(NN1, ncomp, num_layers = 1, dropout = 0.,\n",
        "                         bidirectional = bidi)\n",
        "        \"\"\"\n",
        "        self.rnn = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0,\n",
        "                    bidirectional = bidi, nonlinearity = 'tanh')\n",
        "        self.rnn = nn.GRU(NN1, ncomp, num_layers = 1, dropout = 0,\n",
        "                         bidirectional = bidi)\n",
        "        \"\"\"\n",
        "        \n",
        "        self.mlp = nn.Sequential(\n",
        "                    nn.Linear(ncomp, ncomp*5),\n",
        "                    nn.Mish(),\n",
        "                    nn.Dropout(),\n",
        "                    nn.Linear(ncomp*5, ncomp*5),\n",
        "                    nn.Mish(),\n",
        "                    nn.Dropout(),\n",
        "                    nn.Linear(ncomp*5, ncomp), \n",
        "                    nn.Mish())\n",
        "        \n",
        "        self.fc = nn.Linear(ncomp, NN2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 2, 0)\n",
        "        #print(x.shape)\n",
        "        # h_0 = torch.zeros(2, x.size()[1], self.ncomp).to(device)\n",
        "        \n",
        "        y, h_n = self.rnn(x)\n",
        "\n",
        "        #print(y.shape)\n",
        "        #print(h_n.shape)\n",
        "        if self.rnn.bidirectional:\n",
        "          # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n",
        "          # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n",
        "            q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n",
        "        else:\n",
        "            q = y\n",
        "        \n",
        "        q = self.mlp(q)\n",
        "\n",
        "        # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n",
        "        # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n",
        "        #z = F.softplus(self.fc(q), 10)\n",
        "        #print(q.shape)\n",
        "        z = self.fc(q).permute(2, 0, 1)\n",
        "        # print(z.shape)\n",
        "        return z, q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQGEVQaGmwV6"
      },
      "source": [
        "# we initialize the neural network\n",
        "net = Net(ncomp, NN1, NN2, bidi = True).to(device)\n",
        "\n",
        "# special thing:  we initialize the biases of the last layer in the neural network\n",
        "# we set them as the mean firing rates of the neurons.\n",
        "# this should make the initial predictions close to the mean, because the latents don't contribute much\n",
        "net.fc.bias.data[:] = x1_train.mean(axis = (0,1))\n",
        "\n",
        "# we set up the optimizer. Adjust the learning rate if the training is slow or if it explodes.\n",
        "optimizer1 = torch.optim.Adam(net.parameters(), lr=.0001)\n",
        "# optimizer2 = torch.optim.SGD(net.parameters(), lr = 0.0001, momentum = 0.9, weight_decay = 0.01, )\n",
        "# optimizer3 = torch.optim."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzvKjuTvmwYa"
      },
      "source": [
        "# forward check \n",
        "# net(x1)\n",
        "net(x1_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hlfx8Ltp5jJ"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjzD06B9ta3n"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5REhdgsHZF2",
        "scrolled": true
      },
      "source": [
        "from sam import SAM\n",
        "\n",
        "base_optimizer = torch.optim.Adam  # define an optimizer for the \"sharpness-aware\" update\n",
        "optimizer = SAM(net.parameters(), base_optimizer, lr=0.001, weight_decay = 1e-5)#, momentum=0.9)\n",
        "\n",
        "# you can keep re-running this cell if you think the cost might decrease further\n",
        "\n",
        "cost = nn.MSELoss()\n",
        "\n",
        "#train_save = []\n",
        "#valid_save = []\n",
        "\n",
        "niter =  5500 # \n",
        "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\n",
        "for k in tqdm(range(niter)):\n",
        "    net.train()\n",
        "    # the network outputs the single-neuron prediction and the latents\n",
        "    z, y = net(x1_train)\n",
        "\n",
        "    # our cost\n",
        "    loss = cost(z, x2_train)\n",
        "\n",
        "    # train the network as usual\n",
        "    loss.backward()\n",
        "    optimizer.first_step(zero_grad = True)\n",
        "    \n",
        "    cost(net(x1_train)[0],x2_train).backward()\n",
        "    \n",
        "    optimizer.second_step(zero_grad=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        train_save.append(loss.item())\n",
        "        valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
        "        valid_save.append(valid_loss.item())\n",
        "\n",
        "    if k % 50 == 0:\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
        "            \n",
        "            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4j2gM2HZF2"
      },
      "source": [
        "plt.plot(valid_save)\n",
        "plt.plot(loss_save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24uTE21zHZF2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNG8yY9fp6sg",
        "scrolled": true
      },
      "source": [
        "\"\"\"\n",
        "### Original training\n",
        "# you can keep re-running this cell if you think the cost might decrease further\n",
        "\n",
        "cost = nn.MSELoss()\n",
        "\n",
        "niter = 5800\n",
        "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\n",
        "for k in tqdm(range(niter)):\n",
        "    net.train()\n",
        "    # the network outputs the single-neuron prediction and the latents\n",
        "    z, y = net(x1_train)\n",
        "\n",
        "    # our cost\n",
        "    loss = cost(z, x2_train)\n",
        "\n",
        "    # train the network as usual\n",
        "    loss.backward()\n",
        "    optimizer1.step()\n",
        "    optimizer1.zero_grad()\n",
        "    \n",
        "\n",
        "    if k % 50 == 0:\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
        "            \n",
        "            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')\n",
        "\n",
        "\"\"\";  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkvwOHwjHZF2"
      },
      "source": [
        "## Validation from same neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jryOMKdDJoEh"
      },
      "source": [
        "test, hidden = net(x1_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY_xlS8k7BF8",
        "scrolled": true
      },
      "source": [
        "plt.plot(x2_valid[7,:,0].cpu().detach().numpy())\n",
        "plt.plot(test[7,:,0].cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emfEWsfDHZF3"
      },
      "source": [
        "## Testing neurons from same layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ0dBUgIHZF3"
      },
      "source": [
        "test, hidden = net(torch.from_numpy(avgd_normed_dataRNN[n_neurs:2*n_neurs,:]).unsqueeze(2).to(device).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RORJkXXOHZF3"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZPVIPiOHZF3",
        "scrolled": true
      },
      "source": [
        "n_n = 15\n",
        "plt.plot(test[n_n,:,0].cpu().detach().numpy())\n",
        "plt.plot(avgd_normed_dataRNN[n_neurs + n_n,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1-cMeeHZF4"
      },
      "source": [
        "## Testing neurons from another layer (#9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swaUYZLeHZF4"
      },
      "source": [
        "test, hidden = net(torch.from_numpy(avgd_normed_dataRNN[10000:10100,:]).unsqueeze(2).to(device).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuuFFupaHZF4"
      },
      "source": [
        "plt.plot(test[10,:,0].cpu().detach().numpy())\n",
        "plt.plot(avgd_normed_dataRNN[10010,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-JJAbQHZF4"
      },
      "source": [
        "# Training 9 Networks \n",
        "Each Network corresponds to a different layer in V1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0teh-dlHZF4"
      },
      "source": [
        "# you can keep re-running this cell if you think the cost might decrease further\n",
        "\n",
        "cost = nn.MSELoss()\n",
        "\n",
        "niter = 10000\n",
        "# rnn_loss = 0.2372, lstm_loss = 0.2340, gru_lstm = 0.2370\n",
        "for k in tqdm(range(niter)):\n",
        "    net.train()\n",
        "    # the network outputs the single-neuron prediction and the latents\n",
        "    z, y = net(x1_train)\n",
        "\n",
        "    # our cost\n",
        "    loss = cost(z, x2_train)\n",
        "\n",
        "    # train the network as usual\n",
        "    loss.backward()\n",
        "    optimizer1.step()\n",
        "    optimizer1.zero_grad()\n",
        "\n",
        "    if k % 50 == 0:\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            valid_loss = cost(net(x1_valid)[0], x2_valid)\n",
        "            \n",
        "            print(f' iteration {k}, train cost {loss.item():.4f}, valid cost {valid_loss.item():.4f}')\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kszTdz6bHZF4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}